{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzoriomJBmqHAH7EolO9BB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Milafreire/projeto_extract_datasus_to_bigquery_with_mongodb_pyspark/blob/main/criacao_datawarehouse_datasus_transform_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymongo\n",
        "!pip install pyspark\n",
        "!pip install spark\n",
        "!pip install google.cloud bigquery google.cloud storage"
      ],
      "metadata": {
        "id": "SKPCTLWQqJAP",
        "outputId": "0639d9bb-346c-47b8-ac8d-54f4490fd161",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymongo\n",
            "  Downloading pymongo-4.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (670 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.0/670.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dnspython, pymongo\n",
            "Successfully installed dnspython-2.6.1 pymongo-4.7.2\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=32b7136dc6f50d22c159e6c872c108f233ed8210f6c3d02352ceac6abf7f139d\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n",
            "Collecting spark\n",
            "  Downloading spark-0.2.1.tar.gz (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: spark\n",
            "  Building wheel for spark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spark: filename=spark-0.2.1-py3-none-any.whl size=58749 sha256=5cc96ffc2464f7c3d78f505b35a5da3890fa557351ef4e60a78703126a605f67\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/88/77/b4131110ea4094540f7b47c6d62a649807d7e94800da5eab0b\n",
            "Successfully built spark\n",
            "Installing collected packages: spark\n",
            "Successfully installed spark-0.2.1\n",
            "Collecting google.cloud\n",
            "  Downloading google_cloud-0.34.0-py2.py3-none-any.whl (1.8 kB)\n",
            "Collecting bigquery\n",
            "  Downloading bigquery-0.0.41.tar.gz (8.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting storage\n",
            "  Downloading storage-0.0.4.3.tar.gz (4.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dbstream>=0.1.19 (from bigquery)\n",
            "  Downloading dbstream-0.1.25.tar.gz (4.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: google-cloud-bigquery>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from bigquery) (3.21.0)\n",
            "Collecting googleauthentication>=0.0.12 (from bigquery)\n",
            "  Downloading googleauthentication-0.0.17.tar.gz (2.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: google-cloud-bigquery-storage>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from bigquery) (2.25.0)\n",
            "Collecting pandas==1.3.4 (from bigquery)\n",
            "  Downloading pandas-1.3.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from bigquery) (14.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from pandas==1.3.4->bigquery) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.10/dist-packages (from pandas==1.3.4->bigquery) (2023.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==1.3.4->bigquery) (1.25.2)\n",
            "Collecting paramiko>=1.8.0 (from storage)\n",
            "  Downloading paramiko-3.4.0-py3-none-any.whl (225 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.9/225.9 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dacktool>=0.0.7 (from dbstream>=0.1.19->bigquery)\n",
            "  Downloading dacktool-0.0.7.tar.gz (3.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from dbstream>=0.1.19->bigquery) (2.31.0)\n",
            "Requirement already satisfied: google-api-core>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from dbstream>=0.1.19->bigquery) (2.11.1)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery>=2.4.0->bigquery) (2.27.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery>=2.4.0->bigquery) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery>=2.4.0->bigquery) (2.7.0)\n",
            "Requirement already satisfied: packaging>=20.0.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery>=2.4.0->bigquery) (24.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery-storage>=2.1.0->bigquery) (1.23.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery-storage>=2.1.0->bigquery) (3.20.3)\n",
            "Collecting google-api-python-client==1.7.11 (from googleauthentication>=0.0.12->bigquery)\n",
            "  Downloading google-api-python-client-1.7.11.tar.gz (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.8/142.8 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting google-auth-httplib2==0.0.3 (from googleauthentication>=0.0.12->bigquery)\n",
            "  Downloading google_auth_httplib2-0.0.3-py2.py3-none-any.whl (6.3 kB)\n",
            "Requirement already satisfied: google-auth-oauthlib>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from googleauthentication>=0.0.12->bigquery) (1.2.0)\n",
            "Requirement already satisfied: cryptography>=2.7 in /usr/local/lib/python3.10/dist-packages (from googleauthentication>=0.0.12->bigquery) (42.0.7)\n",
            "Collecting google-cloud-secret-manager==2.7.2 (from googleauthentication>=0.0.12->bigquery)\n",
            "  Downloading google_cloud_secret_manager-2.7.2-py2.py3-none-any.whl (94 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.7.11->googleauthentication>=0.0.12->bigquery) (0.22.0)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==1.7.11->googleauthentication>=0.0.12->bigquery) (1.16.0)\n",
            "Collecting uritemplate<4dev,>=3.0.0 (from google-api-python-client==1.7.11->googleauthentication>=0.0.12->bigquery)\n",
            "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
            "Collecting grpc-google-iam-v1<0.13dev,>=0.12.3 (from google-cloud-secret-manager==2.7.2->googleauthentication>=0.0.12->bigquery)\n",
            "  Downloading grpc_google_iam_v1-0.12.7-py2.py3-none-any.whl (26 kB)\n",
            "Collecting libcst>=0.2.5 (from google-cloud-secret-manager==2.7.2->googleauthentication>=0.0.12->bigquery)\n",
            "  Downloading libcst-1.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bcrypt>=3.2 (from paramiko>=1.8.0->storage)\n",
            "  Downloading bcrypt-4.1.3-cp39-abi3-manylinux_2_28_x86_64.whl (283 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pynacl>=1.5 (from paramiko>=1.8.0->storage)\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=2.7->googleauthentication>=0.0.12->bigquery) (1.16.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core>=2.3.2->dbstream>=0.1.19->bigquery) (1.63.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core>=2.3.2->dbstream>=0.1.19->bigquery) (1.64.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core>=2.3.2->dbstream>=0.1.19->bigquery) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery>=2.4.0->bigquery) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery>=2.4.0->bigquery) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery>=2.4.0->bigquery) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib>=0.4.0->googleauthentication>=0.0.12->bigquery) (1.3.1)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery>=2.4.0->bigquery) (1.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->dbstream>=0.1.19->bigquery) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->dbstream>=0.1.19->bigquery) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->dbstream>=0.1.19->bigquery) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->dbstream>=0.1.19->bigquery) (2024.2.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=2.7->googleauthentication>=0.0.12->bigquery) (2.22)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.9.2->google-api-python-client==1.7.11->googleauthentication>=0.0.12->bigquery) (3.1.2)\n",
            "Requirement already satisfied: pyyaml>=5.2 in /usr/local/lib/python3.10/dist-packages (from libcst>=0.2.5->google-cloud-secret-manager==2.7.2->googleauthentication>=0.0.12->bigquery) (6.0.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery>=2.4.0->bigquery) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.0->googleauthentication>=0.0.12->bigquery) (3.2.2)\n",
            "Building wheels for collected packages: bigquery, storage, dbstream, googleauthentication, google-api-python-client, dacktool\n",
            "  Building wheel for bigquery (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bigquery: filename=bigquery-0.0.41-py3-none-any.whl size=9732 sha256=4c2cb205240965385ea130b2536ccdf3a0cfa0b5bde2bdc7ee7cfbc8490ef476\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/4d/13/d66d9fb04547ef828e6748520a7a6f8faa38f7f0fe6048b7ac\n",
            "  Building wheel for storage (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for storage: filename=storage-0.0.4.3-py3-none-any.whl size=6022 sha256=59b1d4f27e5422bee07eaabf41dabe24281e205c1059f5ed9599ecb7bd1c7911\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/c3/fa/a396db5cf705c32b141fac02d119d2753ec44a91db5acf4fdc\n",
            "  Building wheel for dbstream (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dbstream: filename=dbstream-0.1.25-py3-none-any.whl size=6072 sha256=adbaf321f61f1bad06641c170b1ddc6f5f8f31e8f6dc95539e2c8c2c5a89f063\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/47/49/a0a0839293e3dc4762de6377ae51ad8565d36568d2362aa4d2\n",
            "  Building wheel for googleauthentication (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googleauthentication: filename=googleauthentication-0.0.17-py3-none-any.whl size=3881 sha256=a246681819841e10ed53f7fd599d28a0ca45ca497c4b6b0751b338cda8dd4814\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/4f/26/4b0987274a8684c5e04665179b4b687d240d86eb4aa5a44911\n",
            "  Building wheel for google-api-python-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-api-python-client: filename=google_api_python_client-1.7.11-py3-none-any.whl size=56521 sha256=abd30ce0780ac4f935b8e1ee3a514134e55e26d760844c72539248d937086865\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/77/9d/4196a0a387e67794aeb74cae03c16228f23942199af842f3b2\n",
            "  Building wheel for dacktool (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dacktool: filename=dacktool-0.0.7-py3-none-any.whl size=4782 sha256=858f976564d8977c046ab41292caa9cfe2de144cc6a38ba5b766e0849ad032f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/70/51/5ac14612ee4295b21c4c16b3cc51dceb812c6f8006b6f481b0\n",
            "Successfully built bigquery storage dbstream googleauthentication google-api-python-client dacktool\n",
            "Installing collected packages: google.cloud, uritemplate, libcst, dacktool, bcrypt, pynacl, pandas, paramiko, grpc-google-iam-v1, google-auth-httplib2, storage, google-api-python-client, dbstream, google-cloud-secret-manager, googleauthentication, bigquery\n",
            "  Attempting uninstall: uritemplate\n",
            "    Found existing installation: uritemplate 4.1.1\n",
            "    Uninstalling uritemplate-4.1.1:\n",
            "      Successfully uninstalled uritemplate-4.1.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.0.3\n",
            "    Uninstalling pandas-2.0.3:\n",
            "      Successfully uninstalled pandas-2.0.3\n",
            "  Attempting uninstall: grpc-google-iam-v1\n",
            "    Found existing installation: grpc-google-iam-v1 0.13.0\n",
            "    Uninstalling grpc-google-iam-v1-0.13.0:\n",
            "      Successfully uninstalled grpc-google-iam-v1-0.13.0\n",
            "  Attempting uninstall: google-auth-httplib2\n",
            "    Found existing installation: google-auth-httplib2 0.1.1\n",
            "    Uninstalling google-auth-httplib2-0.1.1:\n",
            "      Successfully uninstalled google-auth-httplib2-0.1.1\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 2.84.0\n",
            "    Uninstalling google-api-python-client-2.84.0:\n",
            "      Successfully uninstalled google-api-python-client-2.84.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydrive2 1.6.3 requires google-api-python-client>=1.12.5, but you have google-api-python-client 1.7.11 which is incompatible.\n",
            "bigframes 1.6.0 requires pandas>=1.5.0, but you have pandas 1.3.4 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 1.3.4 which is incompatible.\n",
            "earthengine-api 0.1.403 requires google-api-python-client>=1.12.1, but you have google-api-python-client 1.7.11 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.0.3, but you have pandas 1.3.4 which is incompatible.\n",
            "mizani 0.9.3 requires pandas>=1.3.5, but you have pandas 1.3.4 which is incompatible.\n",
            "plotnine 0.12.4 requires pandas>=1.5.0, but you have pandas 1.3.4 which is incompatible.\n",
            "statsmodels 0.14.2 requires pandas!=2.1.0,>=1.4, but you have pandas 1.3.4 which is incompatible.\n",
            "xarray 2023.7.0 requires pandas>=1.4, but you have pandas 1.3.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bcrypt-4.1.3 bigquery-0.0.41 dacktool-0.0.7 dbstream-0.1.25 google-api-python-client-1.7.11 google-auth-httplib2-0.0.3 google-cloud-secret-manager-2.7.2 google.cloud-0.34.0 googleauthentication-0.0.17 grpc-google-iam-v1-0.12.7 libcst-1.4.0 pandas-1.3.4 paramiko-3.4.0 pynacl-1.5.0 storage-0.0.4.3 uritemplate-3.0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "86dd933783b548f996df0a96fd6dd450"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Credentials aqui"
      ],
      "metadata": {
        "id": "Fcpi--Tnq0JC"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, explode, to_date\n",
        "from google.cloud.exceptions import NotFound\n",
        "from pymongo import MongoClient\n",
        "import os\n",
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import storage"
      ],
      "metadata": {
        "id": "Jb651CjH7qES"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')\n",
        "\n",
        "os.environ['Google_cloud_projects'] = f'{project_id}'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffazaMNL25Uz",
        "outputId": "41a038c0-c561-4615-9f3d-c20d9908a971"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authenticated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Iniciando as conexões mongodb e spark\n",
        "def get_mongo_client(uri, database_name, collection_name):\n",
        "    client = MongoClient(uri)\n",
        "    db = client[database_name]\n",
        "    return db[collection_name]\n",
        "\n",
        "def create_spark_session(app_name=\"MongoDBToPostgres\"):\n",
        "    return SparkSession.builder \\\n",
        "        .appName(app_name) \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "# Função para ler dados do MongoDB\n",
        "def read_data_from_mongo(collection):\n",
        "    return list(collection.find())\n",
        "\n",
        "# Função para criar DataFrame no Spark\n",
        "def create_spark_dataframe(spark, data):\n",
        "    return spark.createDataFrame(data)"
      ],
      "metadata": {
        "id": "aBZo9Yoy7NSp"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Função para processamento dos dados\n",
        "def process_data(df):\n",
        "    df_exploded = df.select(\n",
        "        col(\"_id\"),\n",
        "        col(\"_index\"),\n",
        "        col(\"_score\"),\n",
        "        col(\"_source.paciente_endereco_coIbgeMunicipio\").alias(\"paciente_endereco_coIbgeMunicipio\"),\n",
        "        col(\"_source.vacina_fabricante_referencia\").alias(\"vacina_fabricante_referencia\"),\n",
        "        col(\"_source.vacina_descricao_dose\").alias(\"vacina_descricao_dose\"),\n",
        "        col(\"_source.vacina_lote\").alias(\"vacina_lote\"),\n",
        "        col(\"_source.vacina_fabricante_nome\").alias(\"vacina_fabricante_nome\"),\n",
        "        col(\"_source.estabelecimento_razaoSocial\").alias(\"razaoSocial\"),\n",
        "        col(\"_source.id_sistema_origem\").alias(\"id_sistema_origem\"),\n",
        "        col(\"_source.document_id\").alias(\"document_id\"),\n",
        "        col(\"_source.vacina_codigo\").alias(\"vacina_codigo\"),\n",
        "        col(\"_source.estabelecimento_valor\").alias(\"estabelecimento_cod\"),\n",
        "        to_date(col(\"_source.data_importacao_rnds\")).alias(\"importacao_rnds\"),\n",
        "        col(\"_source.vacina_grupoAtendimento_codigo\").alias(\"atendimento_cod\"),\n",
        "        col(\"_source.@timestamp\").alias(\"@timestamp\"),\n",
        "        col(\"_source.dt_deleted\").alias(\"dt_deleted\"),\n",
        "        col(\"_source.paciente_endereco_nmMunicipio\").alias(\"paciente_cidade\"),\n",
        "        col(\"_source.vacina_numDose\").alias(\"v_numDose\"),\n",
        "        col(\"_source.paciente_endereco_uf\").alias(\"paciente_uf\"),\n",
        "        col(\"_source.paciente_racaCor_valor\").alias(\"paciente_raca\"),\n",
        "        col(\"_source.co_condicao_maternal\").alias(\"co_condicao_maternal\"),\n",
        "        col(\"_source.data_importacao_datalake\").alias(\"data_importacao_datalake\"),\n",
        "        col(\"_source.paciente_id\").alias(\"paciente_id\"),\n",
        "        col(\"_source.paciente_enumSexoBiologico\").alias(\"paciente_sexo\"),\n",
        "        col(\"_source.@version\").alias(\"@version\"),\n",
        "        col(\"_source.estabelecimento_municipio_nome\").alias(\"estabelecimento_municipio_nome\"),\n",
        "        col(\"_source.estabelecimento_uf\").alias(\"estabelecimento_uf\"),\n",
        "        col(\"_source.paciente_endereco_nmPais\").alias(\"paciente_enderecos\"),\n",
        "        col(\"_source.paciente_endereco_cep\").alias(\"paciente_cep\"),\n",
        "        col(\"_source.vacina_categoria_nome\").alias(\"vacina_categ\"),\n",
        "        col(\"_source.status\").alias(\"status\"),\n",
        "        col(\"_source.paciente_racaCor_codigo\").alias(\"paciente_raca_cod\"),\n",
        "        col(\"_source.sistema_origem\").alias(\"sistema_origem\"),\n",
        "        col(\"_source.paciente_endereco_coPais\").alias(\"pais\"),\n",
        "        col(\"_source.paciente_nacionalidade_enumNacionalidade\").alias(\"nacionalidade\"),\n",
        "        col(\"_source.vacina_grupoAtendimento_nome\").alias(\"v_Atendimento_nome\"),\n",
        "        to_date(col(\"_source.vacina_dataAplicacao\")).alias(\"v_dataAplicacao\"),\n",
        "        col(\"_source.vacina_nome\").alias(\"vacina_nome\"),\n",
        "        col(\"_source.ds_condicao_maternal\").alias(\"ds_condicao_maternal\"),\n",
        "        col(\"_source.vacina_categoria_codigo\").alias(\"vacina_categoria_codigo\"),\n",
        "        col(\"_source.paciente_idade\").alias(\"paciente_idade\"),\n",
        "        col(\"_source.estabelecimento_municipio_codigo\").alias(\"estabelecimento_municipio_codigo\"),\n",
        "        col(\"_source.estalecimento_noFantasia\").alias(\"estalecimento\"),\n",
        "        col(\"_source.paciente_dataNascimento\").alias(\"paciente_nascimento\")\n",
        "    )\n",
        "\n",
        "    df_cleaned = df_exploded.filter(df_exploded.paciente_id.isNotNull()).fillna('')\n",
        "    df_cleaned = df_cleaned.drop(\"_id\", \"_index\", \"_score\", 'dt_deleted', 'atendimento_cod', 'paciente_endereco_coPais', 'status', \"paciente_endereco_coIbgeMunicipio\", \"vacina_fabricante_referencia\", 'vacina_categoria_codigo', 'data_importacao_datalake', 'ds_condicao_maternal', '@version', 'data_importacao_datalake', 'co_condicao_maternal', '@timestamp')\n",
        "\n",
        "    df_cleaned.createOrReplaceTempView(\"datasus_data\")\n",
        "\n",
        "    return df_cleaned"
      ],
      "metadata": {
        "id": "sw72CCmS68NM"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Executar Queries para criação das colunas estado_nome e faixa-etaria\n",
        "def execute_queries(spark, df_cleaned):\n",
        "    result_df1 = spark.sql(\n",
        "        \"\"\"\n",
        "        SELECT *,\n",
        "        CASE\n",
        "            WHEN CAST(paciente_idade AS INT) >= 0 AND CAST(paciente_idade AS INT) <= 9 THEN '0-9'\n",
        "            WHEN CAST(paciente_idade AS INT) >= 10 AND CAST(paciente_idade AS INT) <= 19 THEN '10-19'\n",
        "            WHEN CAST(paciente_idade AS INT) >= 20 AND CAST(paciente_idade AS INT) <= 29 THEN '20-29'\n",
        "            WHEN CAST(paciente_idade AS INT) >= 30 AND CAST(paciente_idade AS INT) <= 39 THEN '30-39'\n",
        "            WHEN CAST(paciente_idade AS INT) >= 40 AND CAST(paciente_idade AS INT) <= 49 THEN '40-49'\n",
        "            WHEN CAST(paciente_idade AS INT) >= 50 AND CAST(paciente_idade AS INT) <= 59 THEN '50-59'\n",
        "            WHEN CAST(paciente_idade AS INT) >= 60 AND CAST(paciente_idade AS INT) <= 69 THEN '60-69'\n",
        "            WHEN CAST(paciente_idade AS INT) >= 70 AND CAST(paciente_idade AS INT) <= 79 THEN '70-79'\n",
        "            WHEN CAST(paciente_idade AS INT) >= 80 AND CAST(paciente_idade AS INT) <= 89 THEN '80-89'\n",
        "            WHEN CAST(paciente_idade AS INT) >= 90 AND CAST(paciente_idade AS INT) <= 99 THEN '90-99'\n",
        "            ELSE '100+'\n",
        "        END AS faixa_etaria\n",
        "        FROM datasus_data\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    result_df2 = spark.sql(\n",
        "        \"\"\"\n",
        "        SELECT *,\n",
        "        CASE\n",
        "            WHEN estabelecimento_uf = 'AC' THEN 'Acre'\n",
        "            WHEN estabelecimento_uf = 'AL' THEN 'Alagoas'\n",
        "            WHEN estabelecimento_uf = 'AP' THEN 'Amapá'\n",
        "            WHEN estabelecimento_uf = 'AM' THEN 'Amazonas'\n",
        "            WHEN estabelecimento_uf = 'BA' THEN 'Bahia'\n",
        "            WHEN estabelecimento_uf = 'CE' THEN 'Ceará'\n",
        "            WHEN estabelecimento_uf = 'DF' THEN 'Distrito Federal'\n",
        "            WHEN estabelecimento_uf = 'ES' THEN 'Espírito Santo'\n",
        "            WHEN estabelecimento_uf = 'GO' THEN 'Goiás'\n",
        "            WHEN estabelecimento_uf = 'MA' THEN 'Maranhão'\n",
        "            WHEN estabelecimento_uf = 'MT' THEN 'Mato Grosso'\n",
        "            WHEN estabelecimento_uf = 'MS' THEN 'Mato Grosso do Sul'\n",
        "            WHEN estabelecimento_uf = 'MG' THEN 'Minas Gerais'\n",
        "            WHEN estabelecimento_uf = 'PA' THEN 'Pará'\n",
        "            WHEN estabelecimento_uf = 'PB' THEN 'Paraíba'\n",
        "            WHEN estabelecimento_uf = 'PR' THEN 'Paraná'\n",
        "            WHEN estabelecimento_uf = 'PE' THEN 'Pernambuco'\n",
        "            WHEN estabelecimento_uf = 'PI' THEN 'Piauí'\n",
        "            WHEN estabelecimento_uf = 'RJ' THEN 'Rio de Janeiro'\n",
        "            WHEN estabelecimento_uf = 'RN' THEN 'Rio Grande do Norte'\n",
        "            WHEN estabelecimento_uf = 'RS' THEN 'Rio Grande do Sul'\n",
        "            WHEN estabelecimento_uf = 'RO' THEN 'Rondônia'\n",
        "            WHEN estabelecimento_uf = 'RR' THEN 'Roraima'\n",
        "            WHEN estabelecimento_uf = 'SC' THEN 'Santa Catarina'\n",
        "            WHEN estabelecimento_uf = 'SP' THEN 'São Paulo'\n",
        "            WHEN estabelecimento_uf = 'SE' THEN 'Sergipe'\n",
        "            WHEN estabelecimento_uf = 'TO' THEN 'Tocantins'\n",
        "            ELSE 'Unknown'\n",
        "        END AS estado_nome\n",
        "        FROM datasus_data\n",
        "        \"\"\"\n",
        "    )\n",
        "    # Renomear colunas nos DataFrames resultantes\n",
        "    result_df1 = result_df1.selectExpr(\n",
        "        \"paciente_id as paciente_id_1\",\n",
        "        \"paciente_sexo as paciente_sexo_1\",\n",
        "        \"paciente_raca as paciente_raca_1\",\n",
        "        \"paciente_cidade as paciente_cidade_1\",\n",
        "        \"nacionalidade as nacionalidade_1\",\n",
        "        \"paciente_idade as paciente_idade_1\",\n",
        "        \"faixa_etaria as faixa_etaria_1\"\n",
        "    )\n",
        "\n",
        "    result_df2 = result_df2.selectExpr(\n",
        "        \"paciente_id as paciente_id_2\",\n",
        "        \"razaoSocial as razaoSocial_2\",\n",
        "        \"estabelecimento_municipio_nome as estabelecimento_municipio_nome_2\",\n",
        "        \"estabelecimento_uf as estabelecimento_uf_2\",\n",
        "        \"estalecimento as estalecimento_2\",\n",
        "        \"estado_nome as estado_nome_2\",\n",
        "        \"vacina_descricao_dose as vacina_descricao_dose_2\",\n",
        "        \"vacina_fabricante_nome as vacina_fabricante_nome_2\",\n",
        "        \"vacina_codigo as vacina_codigo_2\",\n",
        "        \"estabelecimento_cod as estabelecimento_cod_2\",\n",
        "        \"v_numDose as v_numDose_2\",\n",
        "        \"vacina_categ as vacina_categ_2\",\n",
        "        \"v_dataAplicacao as v_dataAplicacao_2\",\n",
        "        \"vacina_nome as vacina_nome_2\"\n",
        "    )\n",
        "\n",
        "    #Join para unir dos dfs resultantes\n",
        "    combined_df = result_df1.join(result_df2, result_df1['paciente_id_1'] == result_df2['paciente_id_2'])\n",
        "\n",
        "    # Selecionar colunas finais do DataFrame combinado\n",
        "    final_df = combined_df.select(\n",
        "      col('paciente_id_1').alias('paciente_id'),\n",
        "      col('paciente_sexo_1').alias('paciente_sexo'),\n",
        "      col('paciente_raca_1').alias('paciente_raca'),\n",
        "      col('paciente_cidade_1').alias('paciente_cidade'),\n",
        "      col('nacionalidade_1').alias('nacionalidade'),\n",
        "      col('paciente_idade_1').alias('paciente_idade'),\n",
        "      col('faixa_etaria_1').alias('faixa_etaria'),\n",
        "      col('razaoSocial_2').alias('razaoSocial'),\n",
        "      col('estabelecimento_municipio_nome_2').alias('estabelecimento_municipio_nome'),\n",
        "      col('estabelecimento_uf_2').alias('estabelecimento_uf'),\n",
        "      col('estalecimento_2').alias('estalecimento'),\n",
        "      col('estado_nome_2').alias('estado_nome'),\n",
        "      col('vacina_descricao_dose_2').alias('vacina_descricao_dose'),\n",
        "      col('vacina_fabricante_nome_2').alias('vacina_fabricante_nome'),\n",
        "      col('vacina_codigo_2').alias('vacina_codigo'),\n",
        "      col('estabelecimento_cod_2').alias('estabelecimento_cod'),\n",
        "      col('v_numDose_2').alias('v_numDose'),\n",
        "      col('vacina_categ_2').alias('vacina_categ'),\n",
        "      col('v_dataAplicacao_2').alias('v_dataAplicacao'),\n",
        "      col('vacina_nome_2').alias('vacina_nome')\n",
        "  )\n",
        "\n",
        "    # Converter DataFrame final para Pandas\n",
        "    final_pandas_df = final_df.toPandas()"
      ],
      "metadata": {
        "id": "SdCl2ajx7_f7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Schemas\n",
        "schema_dim_paciente = [\n",
        "    bigquery.SchemaField(\"paciente_id\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"paciente_sexo\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"paciente_raca\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"paciente_cidade\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"paciente_nacionalidade\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"paciente_idade\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"faixa_etaria\", \"STRING\")\n",
        "]\n",
        "\n",
        "schema_dim_localizacao = [\n",
        "    bigquery.SchemaField(\"paciente_id\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"razaoSocial\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"estabelecimento_municipio_nome\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"estabelecimento_uf\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"estalecimento\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"estado_nome\", \"STRING\")\n",
        "]\n",
        "\n",
        "schema_fato_atendimento = [\n",
        "    bigquery.SchemaField(\"vacina_descricao_dose\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"vacina_fabricante_nome\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"paciente_id\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"vacina_codigo\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"estabelecimento_cod\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"v_numDose\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"vacina_categ\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"v_dataAplicacao\", \"DATE\"),\n",
        "    bigquery.SchemaField(\"vacina_nome\", \"STRING\")\n",
        "]"
      ],
      "metadata": {
        "id": "Tb7oxxMX91UH"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para criar tabela se não existir\n",
        "def create_table_if_not_exists(client, table_id, schema):\n",
        "    try:\n",
        "        client.get_table(table_id)\n",
        "        print(f\"Table {table_id} already exists.\")\n",
        "    except NotFound:\n",
        "        table = bigquery.Table(table_id, schema=schema)\n",
        "        client.create_table(table)\n",
        "        print(f\"Table {table_id} created.\")"
      ],
      "metadata": {
        "id": "rIR0W19T9RX1"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializar cliente BigQuery\n",
        "client = bigquery.Client(project=project_id)"
      ],
      "metadata": {
        "id": "UlpAorMG_Dci"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Om8ydvNJ1ji4"
      },
      "outputs": [],
      "source": [
        "#Função para inserção dos dados no Big query\n",
        "def load_parquet_to_bq(client, uri, table_id, schema):\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        source_format=bigquery.SourceFormat.PARQUET,\n",
        "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE\n",
        "    )\n",
        "    load_job = client.load_table_from_uri(uri, table_id, job_config=job_config)\n",
        "    load_job.result()\n",
        "    destination_table = client.get_table(table_id)\n",
        "    print(f\"Loaded {destination_table.num_rows} rows to {table_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IDs das tabelas\n",
        "table_id_dim_paciente = f\"{project_id}.{dataset_id}.dim_paciente\"\n",
        "table_id_dim_localizacao = f\"{project_id}.{dataset_id}.dim_localizacao\"\n",
        "table_id_fato_atendimento = f\"{project_id}.{dataset_id}.fato_atendimento\"\n",
        "\n",
        "# URIs dos arquivos Parquet no GCS\n",
        "gcs_uri_dim_paciente = f\"gs://{pasta_storage}/parquet_files/dim_paciente.parquet\"\n",
        "gcs_uri_dim_localizacao = f\"gs://{pasta_storage}/parquet_files/dim_localizacao.parquet\"\n",
        "gcs_uri_fato_atendimento = f\"gs://{pasta_storage}/parquet_files/fato_atendimento.parquet\"\n",
        "\n",
        "# Criar tabelas no BigQuery se não existirem\n",
        "create_table_if_not_exists(client, table_id_dim_paciente, schema_dim_paciente)\n",
        "create_table_if_not_exists(client, table_id_dim_localizacao, schema_dim_localizacao)\n",
        "create_table_if_not_exists(client, table_id_fato_atendimento, schema_fato_atendimento)\n",
        "\n",
        "# Carregar arquivos Parquet no BigQuery\n",
        "load_parquet_to_bq(client, gcs_uri_dim_paciente, table_id_dim_paciente, schema_dim_paciente)\n",
        "load_parquet_to_bq(client, gcs_uri_dim_localizacao, table_id_dim_localizacao, schema_dim_localizacao)\n",
        "load_parquet_to_bq(client, gcs_uri_fato_atendimento, table_id_fato_atendimento, schema_fato_atendimento)"
      ],
      "metadata": {
        "id": "sqZjSyXf_Sdq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59e1dd87-2e45-40b0-bf39-70f4f3098cc2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Table mywebscrap-423316.estudos_gcp.dim_paciente already exists.\n",
            "Table mywebscrap-423316.estudos_gcp.dim_localizacao already exists.\n",
            "Table mywebscrap-423316.estudos_gcp.fato_atendimento already exists.\n",
            "Loaded 14544 rows to mywebscrap-423316.estudos_gcp.dim_paciente\n",
            "Loaded 14544 rows to mywebscrap-423316.estudos_gcp.dim_localizacao\n",
            "Loaded 14544 rows to mywebscrap-423316.estudos_gcp.fato_atendimento\n"
          ]
        }
      ]
    }
  ]
}